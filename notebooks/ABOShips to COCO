{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOBWhGBs7ssjnclJmVntPn5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# If Pillow isn't installed:\n","%pip -q install pillow\n","\n","from pathlib import Path\n","import csv, json, random, shutil\n","from typing import Dict, List, Tuple, Set\n","from PIL import Image\n","\n","# Source dataset (sibling of the notebook)\n","ROOT = Path(\"ABOShipsDataset\")              # contains Seaships/ and Labels/\n","CSV_PATH = ROOT / \"Labels\" / \"Vesibussi_Labels.csv\"\n","IMAGES_DIR = ROOT / \"Seaships\"              # date-named subfolders with images\n","\n","# Fresh output dataset (WILL contain real copied images)\n","OUT_DIR = Path(\"coco_aboships_copy\")\n","\n","# Overwrite the output directory if it already exists?\n","OVERWRITE_OUT_DIR = True\n","\n","# Random split ratios (must sum to 1.0)\n","SPLITS = {\"train\": 0.8, \"valid\": 0.1, \"test\": 0.1}\n","RANDOM_SEED = 42\n","\n","# Optionally drop labels (lowercase, no spaces), e.g. {\"seamark\",\"miscellaneous\"}\n","DROP_CLASSES: Set[str] = set()\n","\n","# OPTIONAL: split by date folder names instead of random to avoid near-duplicates.\n","USE_DATE_SPLIT = False\n","DATE_SPLIT = {\n","    \"train\": [\"20180626\",\"20180627\",\"20180628\",\"20180629\",\"20180630\",\"20180701\",\"20180702\",\"20180703\",\"20180704\"],\n","    \"valid\": [\"20180705\"],\n","    \"test\":  [\"20180708\"]\n","}\n","\n","IMG_EXTS = {\".png\", \".jpg\", \".jpeg\"}\n","\n","def norm_class(name: str) -> str:\n","    return name.strip().lower().replace(\" \", \"\")\n","\n","def scan_images(images_root: Path) -> Dict[str, Path]:\n","    \"\"\"Map image stem -> absolute path by scanning Seaships/* subfolders.\"\"\"\n","    stem2path = {}\n","    for p in images_root.rglob(\"*\"):\n","        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n","            stem2path.setdefault(p.stem, p.resolve())\n","    return stem2path\n","\n","def read_csv_rows(csv_path: Path) -> List[dict]:\n","    \"\"\"Read CSV and return rows with (stem, cls, xmin,xmax,ymin,ymax).\"\"\"\n","    rows = []\n","    with open(csv_path, newline=\"\") as f:\n","        reader = csv.DictReader(f)\n","        fmap = {k.lower(): k for k in (reader.fieldnames or [])}\n","\n","        def get(row, *names):\n","            for n in names:\n","                k = fmap.get(n.lower())\n","                if k is not None and row.get(k, \"\") != \"\":\n","                    return row[k]\n","            return None\n","\n","        for r in reader:\n","            fname = get(r, \"filename\", \"image\", \"img\", \"img_id\")\n","            cls_raw = get(r, \"class\", \"label\", \"category\")\n","            xmin = get(r, \"xmin\", \"x_min\", \"left\")\n","            xmax = get(r, \"xmax\", \"x_max\", \"right\")\n","            ymin = get(r, \"ymin\", \"y_min\", \"top\")\n","            ymax = get(r, \"ymax\", \"y_max\", \"bottom\")\n","            if not (fname and cls_raw and xmin and xmax and ymin and ymax):\n","                continue\n","            try:\n","                x1, x2 = float(xmin), float(xmax)\n","                y1, y2 = float(ymin), float(ymax)\n","            except Exception:\n","                continue\n","            rows.append({\n","                \"stem\": Path(fname).stem,\n","                \"cls\": norm_class(cls_raw),\n","                \"xmin\": x1, \"xmax\": x2, \"ymin\": y1, \"ymax\": y2\n","            })\n","    return rows\n","\n","def get_image_size(p: Path, cache: Dict[Path, Tuple[int,int]]) -> Tuple[int,int]:\n","    if p in cache:\n","        return cache[p]\n","    with Image.open(p) as im:\n","        cache[p] = im.size\n","    return cache[p]\n","\n","def safe_bbox(x, y, w, h, W, H):\n","    \"\"\"Clip to image bounds and enforce min size 1x1.\"\"\"\n","    x = max(0.0, min(x, W - 1.0))\n","    y = max(0.0, min(y, H - 1.0))\n","    w = max(1.0, min(w, W - x))\n","    h = max(1.0, min(h, H - y))\n","    return [x, y, w, h]\n","\n","def coco_skeleton(categories):\n","    return {\n","        \"info\": {\"description\": \"ABOShips → COCO (copied images)\", \"version\": \"1.0\"},\n","        \"licenses\": [],\n","        \"images\": [],\n","        \"annotations\": [],\n","        \"categories\": categories\n","    }\n","\n","# sanity checks\n","assert IMAGES_DIR.exists(), f\"Missing: {IMAGES_DIR}\"\n","assert CSV_PATH.exists(), f\"Missing: {CSV_PATH}\"\n","\n","# (re)create output\n","if OUT_DIR.exists() and OVERWRITE_OUT_DIR:\n","    shutil.rmtree(OUT_DIR)\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","random.seed(RANDOM_SEED)\n","\n","# read source\n","rows = read_csv_rows(CSV_PATH)\n","stem2path = scan_images(IMAGES_DIR)\n","\n","# filter rows\n","drops = {c.lower().replace(\" \",\"\") for c in DROP_CLASSES}\n","filtered = []\n","missing = set()\n","for r in rows:\n","    if r[\"cls\"] in drops:\n","        continue\n","    if r[\"stem\"] not in stem2path:\n","        missing.add(r[\"stem\"])\n","        continue\n","    filtered.append(r)\n","\n","print(f\"CSV rows: {len(rows)} | kept: {len(filtered)} | missing images by stem: {len(missing)}\")\n","if missing:\n","    print(\"  e.g.:\", next(iter(missing)))\n","\n","# unique image stems with ≥1 kept annotation\n","stems = sorted({r[\"stem\"] for r in filtered})\n","\n","# split mapping\n","if USE_DATE_SPLIT:\n","    stem_date = {st: stem2path[st].parent.name for st in stems}\n","    split_map = {}\n","    for sp, date_list in DATE_SPLIT.items():\n","        for st, d in stem_date.items():\n","            if d in date_list:\n","                split_map[st] = sp\n","    for st in stems:\n","        split_map.setdefault(st, \"train\")\n","else:\n","    tmp = stems[:]\n","    random.shuffle(tmp)\n","    n = len(tmp)\n","    n_train = int(SPLITS[\"train\"] * n)\n","    n_valid = int(SPLITS[\"valid\"] * n)\n","    train_set = set(tmp[:n_train])\n","    valid_set = set(tmp[n_train:n_train+n_valid])\n","    split_map = {st: (\"train\" if st in train_set else \"valid\" if st in valid_set else \"test\")\n","                 for st in stems}\n","\n","# ====== KEY FIXES ======\n","# 1) zero-based, contiguous class IDs (0..N-1)\n","# 2) non-\"none\" supercategory (so RF-DETR doesn't filter them out)\n","classes = sorted({r[\"cls\"] for r in filtered})\n","cat2id = {c: i for i, c in enumerate(classes)}  # <-- zero-based\n","categories = [{\"id\": cat2id[c], \"name\": c, \"supercategory\": \"vessel\"} for c in classes]\n","print(\"Classes (zero-based):\", cat2id)\n","\n","# build COCO dicts\n","splits = (\"train\",\"valid\",\"test\")\n","bases = {s: coco_skeleton(categories) for s in splits}\n","img_id_counter = {s: 0 for s in splits}\n","stem2imgid = {s: {} for s in splits}\n","size_cache: Dict[Path, Tuple[int,int]] = {}\n","ann_id = 1\n","skipped_small = 0\n","\n","for r in filtered:\n","    st = r[\"stem\"]\n","    s = split_map[st]\n","    img_path = stem2path[st]\n","    W, H = get_image_size(img_path, size_cache)\n","\n","    # add image once per split\n","    if st not in stem2imgid[s]:\n","        img_id_counter[s] += 1\n","        iid = img_id_counter[s]\n","        stem2imgid[s][st] = iid\n","        bases[s][\"images\"].append({\n","            \"id\": iid,\n","            \"file_name\": img_path.name,  # copy will place this in the split dir\n","            \"width\": W,\n","            \"height\": H\n","        })\n","    else:\n","        iid = stem2imgid[s][st]\n","\n","    # bbox to [x,y,w,h] with clipping\n","    x1, x2, y1, y2 = r[\"xmin\"], r[\"xmax\"], r[\"ymin\"], r[\"ymax\"]\n","    x, y = min(x1, x2), min(y1, y2)\n","    w, h = abs(x2 - x1), abs(y2 - y1)\n","    bbox = safe_bbox(x, y, w, h, W, H)\n","\n","    if bbox[2] < 1 or bbox[3] < 1:\n","        skipped_small += 1\n","        continue\n","\n","    bases[s][\"annotations\"].append({\n","        \"id\": ann_id,\n","        \"image_id\": iid,\n","        \"category_id\": cat2id[r[\"cls\"]],   # <-- zero-based ID\n","        \"bbox\": bbox,\n","        \"area\": bbox[2] * bbox[3],\n","        \"iscrowd\": 0\n","    })\n","    ann_id += 1\n","\n","if skipped_small:\n","    print(\"Skipped tiny/degenerate boxes:\", skipped_small)\n","\n","# write JSONs and COPY images\n","for s in splits:\n","    split_dir = OUT_DIR / s\n","    split_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # copy only used images\n","    used_stems = set(stem2imgid[s].keys())\n","    for st in used_stems:\n","        src = stem2path[st]\n","        dst = split_dir / src.name\n","        if not dst.exists():  # skip if already copied\n","            shutil.copy2(src, dst)\n","\n","    with open(split_dir / \"_annotations.coco.json\", \"w\") as f:\n","        json.dump(bases[s], f, indent=2)\n","\n","    print(f\"{s}: {len(bases[s]['images'])} images, {len(bases[s]['annotations'])} ann, {len(categories)} classes\")\n","\n","print(f\"\\nDone → {OUT_DIR.resolve()}\")\n","print(\"\"\"\n","<out>/\n","  train/  _annotations.coco.json  *.png|*.jpg\n","  valid/  _annotations.coco.json  *.png|*.jpg\n","  test/   _annotations.coco.json  *.png|*.jpg\n","\"\"\".replace(\"<out>\", OUT_DIR.name))\n","\n","# --- quick validator ---\n","def validate(split_dir: Path):\n","    d = json.load(open(split_dir / \"_annotations.coco.json\"))\n","    ids = sorted(c[\"id\"] for c in d[\"categories\"])\n","    assert ids == list(range(len(ids))), f\"{split_dir.name}: category ids must be 0..N-1, got {ids}\"\n","    cat_set = set(ids)\n","    bad = [a for a in d[\"annotations\"] if a[\"category_id\"] not in cat_set]\n","    print(f\"{split_dir.name}: OK | bad category refs: {len(bad)}\")\n","\n","for s in splits:\n","    validate(OUT_DIR / s)\n"],"metadata":{"id":"MSBLzdIj5Mr9"},"execution_count":null,"outputs":[]}]}